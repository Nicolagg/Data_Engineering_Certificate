from airflow import DAG
from datetime import timedelta
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
from airflow.operators.python import PythonOperator
import urllib.request
import time
import glob
import os
import json


def delay(seconds):
    time.sleep(seconds)

def catalog():
    # Define pull(url) helper function
    def pull(url):
        # Code to pull data from the URL and assign it to the 'data' variable
        # request json file
        response = urllib.request.urlopen(url).read()
        data = response.decode('utf-8')

        return data

    # Define store(data, file) helper function
    def store(data, file):
        # Create and open a file named after the URL where the data came from
        with open(file, 'w') as f:
            # Write the data to the file
            f.write(data)

        # Print a message indicating the file was written
        print('Wrote file: ' + file)


    # Read URLs from file and store them in a list
    urls = []
    with open('/opt/airflow/dags/00_urls.txt', 'r') as file:
        for line in file:
            urls.append(line.strip())
    
    
    
    # Loop through the URLs
    for url in urls:
        # Call the pull(url) function to obtain the data
        data = pull(url)

        # Extract the filename from the URL
        index = url.rfind('/') + 1
        file = url[index:]

        # Call the store(data, file) function to store the data
        store(data, file)

        # Print messages
        print('Pulled: ' + file)
        print('--- Waiting ---\n###########')
        delay(15)


def combine():
    # Open the output file in write mode
    with open('combo.txt', 'w') as outfile:
        # Loop through each file with the extension '.html' in the current directory
        for file in glob.glob("*.html"):
            # Open each input file in read mode
            with open(file) as infile:
                # Read the contents of the input file and write them to the output file
                outfile.write(infile.read())

def titles():
    from bs4 import BeautifulSoup

    # Define the helper function to store JSON data
    def store_json(data,file):
        with open(file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            print('wrote file: ' + file)


    # Open and read the large HTML file generated by combine()
    with open('combo.txt', 'r') as f:
        html = f.read()

    # Replace new line and carriage return characters in the HTML
    html = html.replace('\n', ' ').replace('\r', '')

    # Create an HTML parser using BeautifulSoup
    soup = BeautifulSoup(html, "html.parser")
    results = soup.find_all('h3')
    titles = []

    # Extract the text from h3 elements
    for item in results:
        titles.append(item.text)

    # Store the titles in a JSON file
    store_json(titles, 'titles.json')



def clean():
    # Define the helper function to store JSON data
    def store_json(data, file):
        with open(file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            print('Wrote file: ' + file)

    with open('titles.json') as file:
        titles = json.load(file)

        # Remove punctuation/numbers
        for index, title in enumerate(titles):
            punctuation = '''!()-[]{};:'"\,<>./?@#$%^&*_~1234567890'''
            translationTable = str.maketrans("", "", punctuation)
            clean = title.translate(translationTable)
            titles[index] = clean

        # Remove one-character words
        for index, title in enumerate(titles):
            clean = ' '.join([word for word in title.split() if len(word) > 1])
            titles[index] = clean

        store_json(titles, 'titles_clean.json')


def count_words():
    from collections import Counter

     # Define the helper function to store JSON data
    def store_json(data,file):
        with open(file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            print('Wrote file: ' + file)
    # Open the 'titles_clean.json' file

    with open('titles_clean.json') as file:
            # Load the JSON data into the 'titles' variable
            titles = json.load(file)
            words = []

            # extract words and flatten
            for title in titles:
                words.extend(title.split())

            # count word frequency
            counts = Counter(words)

            # Store the word frequency counts in a JSON file
            store_json(counts, 'words.json')



with DAG(
   "assignment",
   start_date=days_ago(1), 
   schedule_interval="@daily",
   catchup=False,
) as dag:

    # Install BeautifulSoup4 using a bash command
    t0 = BashOperator(
        task_id='task_zero',
        bash_command='pip install beautifulsoup4',
        retries=2
    )

    # Call the catalog() function
    t1 = PythonOperator(
        task_id='task_one',
        depends_on_past=False,
        python_callable=catalog,
        execution_timeout=timedelta(minutes=30),
        
    )

    # Call the combine() function
    t2 = PythonOperator(
        task_id='task_two',
        depends_on_past=False,
        python_callable=combine
    )

    # Call the titles() function
    t3 = PythonOperator(
        task_id='task_three',
        depends_on_past=False,
        python_callable=titles
    )

    # Call the clean() function
    t4 = PythonOperator(
        task_id='task_four',
        depends_on_past=False,
        python_callable=clean
    )

    # Call the count_words() function
    t5 = PythonOperator(
        task_id='task_five',
        depends_on_past=False,
        python_callable=count_words
    )

    # Define the task dependencies
    t0 >> t1 >> t2 >> t3 >> t4 >> t5
